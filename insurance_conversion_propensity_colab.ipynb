{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Insurance Conversion Propensity Modeling (Colab)\n\nEnd-to-end notebook for ranking users by conversion propensity (`has_sale`) with **Average Precision (PR-AUC)** as the primary model-selection metric."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0 — Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Colab-friendly installs\n!pip -q install lightgbm imbalanced-learn optuna optuna-integration shap joblib\n\nimport os\nimport random\nimport warnings\nfrom dataclasses import dataclass\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport shap\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom optuna.integration import lightgbm as optuna_lgb\nfrom scipy import sparse\nfrom sklearn.base import clone\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    average_precision_score,\n    brier_score_loss,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nwarnings.filterwarnings('ignore')\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nprint('Seed set to', SEED)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Optional: choose CSV from your local machine (Colab upload popup)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If running in Colab, this opens a file upload popup.\n# If you skip it, notebook uses default DATA_PATH = 'data_(2).csv'.\nDATA_PATH = 'data_(2).csv'\n\ntry:\n    from google.colab import files\n    uploaded = files.upload()  # popup selector\n    if uploaded:\n        DATA_PATH = next(iter(uploaded.keys()))\n        print('Using uploaded file:', DATA_PATH)\n    else:\n        print('No file uploaded. Using default:', DATA_PATH)\nexcept Exception:\n    print('Not in Colab (or upload skipped). Using default:', DATA_PATH)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1 — Load + sanity checks"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = pd.read_csv(DATA_PATH)\ndf['dt'] = pd.to_datetime(df['dt'])\n\nprint('Shape:', df.shape)\nprint('\nColumns:', list(df.columns))\nprint('\nDtypes:\n', df.dtypes)\n\nmissing_tbl = df.isna().mean().sort_values(ascending=False).rename('missing_rate')\nprint('\nMissingness (%):\n', (missing_tbl * 100).round(2))\n\nbefore_n, before_pos = len(df), int(df['has_sale'].sum())\ndup_mask = df.duplicated(keep='first')\nnum_dups = int(dup_mask.sum())\ndf = df.loc[~dup_mask].copy()\nafter_n, after_pos = len(df), int(df['has_sale'].sum())\n\nprint(f\"\\nRemoved duplicates: {num_dups}\")\nprint(f\"Rows: {before_n} -> {after_n}\")\nprint(f\"Positive count: {before_pos} -> {after_pos}\")\nprint(f\"Positive rate: {before_n and before_pos/before_n:.4f} -> {after_n and after_pos/after_n:.4f}\")\n\n# per provided data facts\nassert num_dups == 164, f'Expected 164 duplicates, got {num_dups}'\n\nuniq_comm = df['commission_rate'].nunique(dropna=False)\nprint('commission_rate unique values:', uniq_comm)\nassert uniq_comm == 1, 'commission_rate expected constant.'\n\ndf = df.drop(columns=['commission_rate'])\nprint('Dropped commission_rate. New shape:', df.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2 — EDA with figures (matplotlib)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\ncounts = df['has_sale'].value_counts().sort_index()\naxes[0].bar(['No Sale (0)', 'Sale (1)'], counts.values, color=['#4C72B0', '#55A868'])\naxes[0].set_title('Target Counts')\naxes[0].set_ylabel('Count')\n\nrate = df['has_sale'].mean()\naxes[1].bar(['Conversion Rate'], [rate], color='#55A868')\naxes[1].set_ylim(0, 1)\naxes[1].set_title(f'Conversion Rate: {rate:.2%}')\naxes[1].set_ylabel('Rate')\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "missing = df.isna().mean().sort_values(ascending=False)\nmissing = missing[missing > 0]\nplt.figure(figsize=(7,4))\nplt.bar(missing.index, missing.values*100, color='#C44E52')\nplt.title('Missingness by Column (%)')\nplt.ylabel('Missing %')\nplt.xticks(rotation=30, ha='right')\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tmp = df.copy()\ntmp['platform'] = tmp['platform'].fillna('Missing')\nrate_by_platform = tmp.groupby('platform')['has_sale'].mean().sort_values(ascending=False)\nplt.figure(figsize=(6,4))\nplt.bar(rate_by_platform.index, rate_by_platform.values, color='#8172B2')\nplt.title('Conversion Rate by Platform')\nplt.ylabel('Conversion rate')\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tmp = df.copy()\ntmp['membership_level'] = tmp['membership_level'].fillna('Missing')\nrate_by_member = tmp.groupby('membership_level')['has_sale'].mean().sort_values(ascending=False)\nplt.figure(figsize=(7,4))\nplt.bar(rate_by_member.index, rate_by_member.values, color='#64B5CD')\nplt.title('Conversion Rate by Membership Level (incl. Missing)')\nplt.ylabel('Conversion rate')\nplt.xticks(rotation=20)\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "num_cols = ['age','monthly_cost','session_time','household_income']\nfig, axes = plt.subplots(2,2, figsize=(14,8))\naxes = axes.flatten()\nfor ax, c in zip(axes, num_cols):\n    ax.hist(df.loc[df['has_sale']==0,c], bins=30, density=True, alpha=0.55, label='No Sale')\n    ax.hist(df.loc[df['has_sale']==1,c], bins=30, density=True, alpha=0.55, label='Sale')\n    ax.set_title(f'{c} by target')\n    ax.legend()\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "fig, axes = plt.subplots(2,2, figsize=(14,8))\naxes = axes.flatten()\nfor ax, c in zip(axes, num_cols):\n    b = pd.qcut(df[c], q=10, duplicates='drop')\n    yb = df.groupby(b)['has_sale'].mean().reset_index(drop=True)\n    ax.plot(range(len(yb)), yb.values, marker='o')\n    ax.set_title(f'Binned conversion rate vs {c}')\n    ax.set_xlabel('Quantile bin')\n    ax.set_ylabel('Conversion rate')\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Quick insights:** conversion is imbalanced (~22%), platform is a strong differentiator, and some numeric features show non-linear conversion patterns."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3 — Feature engineering + preprocessing (sklearn Pipeline)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "EPS = 1e-6\nTARGET = 'has_sale'\nID_COL = 'id'\n\ndef add_features(frame: pd.DataFrame) -> pd.DataFrame:\n    out = frame.copy()\n    out['day_of_week'] = out['dt'].dt.dayofweek\n    out['is_weekend'] = (out['day_of_week'] >= 5).astype(int)\n    out['day_of_month'] = out['dt'].dt.day\n    out['month'] = out['dt'].dt.month\n    out['cost_income_ratio'] = out['monthly_cost'] / np.maximum(out['household_income'].values, EPS)\n    return out\n\nwork_df = add_features(df)\nDROP_COLS = [TARGET, ID_COL, 'dt']\nX_all = work_df.drop(columns=DROP_COLS)\ny_all = work_df[TARGET].astype(int)\n\nnum_features = X_all.select_dtypes(include=['number']).columns.tolist()\ncat_features = X_all.select_dtypes(exclude=['number']).columns.tolist()\n\npreprocessor_scaled = ColumnTransformer([\n    ('num', Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ]), num_features),\n    ('cat', Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n    ]), cat_features)\n])\n\n# no scaler path for tree-based models / LightGBM tuner matrices\npreprocessor_tree = ColumnTransformer([\n    ('num', Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n    ]), num_features),\n    ('cat', Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n    ]), cat_features)\n])\n\nprint('Numeric features:', num_features)\nprint('Categorical features:', cat_features)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4 — Modeling & evaluation (ranking-focused)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def evaluate(y_true, y_score):\n    return {\n        'pr_auc': average_precision_score(y_true, y_score),\n        'roc_auc': roc_auc_score(y_true, y_score)\n    }\n\ndef precision_recall_at_k(y_true, y_score, k_frac):\n    y_true = np.asarray(y_true)\n    y_score = np.asarray(y_score)\n    n = len(y_true)\n    k = max(1, int(np.floor(n*k_frac)))\n    idx = np.argsort(-y_score)[:k]\n    precision = y_true[idx].mean()\n    recall = y_true[idx].sum() / y_true.sum()\n    threshold = np.sort(y_score)[-k]\n    return {'k_frac':k_frac, 'k':k, 'threshold':float(threshold), 'precision@k':float(precision), 'recall@k':float(recall)}\n\n# STRICT no-leak split:\n# 1) train_val/test split (test untouched until final evaluation)\n# 2) train/valid split inside train_val for model/tuner selection\nX_train_val, X_test, y_train_val, y_test, idx_train_val, idx_test = train_test_split(\n    X_all, y_all, work_df.index, test_size=0.2, stratify=y_all, random_state=SEED\n)\nX_train, X_valid, y_train, y_valid, idx_train, idx_valid = train_test_split(\n    X_train_val, y_train_val, idx_train_val, test_size=0.25, stratify=y_train_val, random_state=SEED\n)\nprint('Split sizes -> train:',len(X_train),'valid:',len(X_valid),'test:',len(X_test))\n\nmodels = {\n    'LogisticRegression': Pipeline([\n        ('prep', preprocessor_scaled),\n        ('model', LogisticRegression(max_iter=3000, class_weight='balanced', random_state=SEED))\n    ]),\n    'BalancedRandomForest': Pipeline([\n        ('prep', preprocessor_tree),\n        ('model', BalancedRandomForestClassifier(n_estimators=400, min_samples_leaf=2, random_state=SEED, n_jobs=-1))\n    ]),\n    'LGBM_baseline': Pipeline([\n        ('prep', preprocessor_tree),\n        ('model', LGBMClassifier(n_estimators=400, learning_rate=0.05, num_leaves=31,\n                                 class_weight='balanced', random_state=SEED, n_jobs=-1, verbosity=-1))\n    ])\n}\n\nvalid_scores = {}\nrows = []\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    p_valid = model.predict_proba(X_valid)[:,1]\n    met = evaluate(y_valid, p_valid)\n    valid_scores[name] = p_valid\n    rows.append({'model':name, **met})\n\npd.DataFrame(rows).sort_values('pr_auc', ascending=False)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Alternative time-based split for robustness comparison\ntmp = work_df.sort_values('dt').reset_index(drop=False)\ncut = int(len(tmp)*0.8)\ntrain_t = tmp.iloc[:cut]\ntest_t = tmp.iloc[cut:]\n\nXtr_t = train_t.drop(columns=DROP_COLS + ['index'])\nytr_t = train_t[TARGET].astype(int)\nXte_t = test_t.drop(columns=DROP_COLS + ['index'])\nyte_t = test_t[TARGET].astype(int)\n\nrows = []\nfor name, model in models.items():\n    m = clone(model)\n    m.fit(Xtr_t, ytr_t)\n    s = m.predict_proba(Xte_t)[:,1]\n    rows.append({'model':name, **evaluate(yte_t, s)})\n\nprint('Time-based split performance:')\npd.DataFrame(rows).sort_values('pr_auc', ascending=False)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Curves on validation set for baseline models\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nfor name, score in valid_scores.items():\n    p, r, _ = precision_recall_curve(y_valid, score)\n    plt.plot(r, p, label=f\"{name} (AP={average_precision_score(y_valid, score):.3f})\")\nplt.title('Validation Precision-Recall Curves')\nplt.xlabel('Recall'); plt.ylabel('Precision'); plt.legend()\n\nplt.subplot(1,2,2)\nfor name, score in valid_scores.items():\n    fpr, tpr, _ = roc_curve(y_valid, score)\n    plt.plot(fpr, tpr, label=f\"{name} (ROC-AUC={roc_auc_score(y_valid, score):.3f})\")\nplt.plot([0,1],[0,1],'k--',alpha=0.5)\nplt.title('Validation ROC Curves')\nplt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend()\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5 — Hyperparameter tuning (optimize PR-AUC)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5A) Balanced Random Forest tuning (RandomizedSearchCV)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "brf_pipe = Pipeline([\n    ('prep', preprocessor_tree),\n    ('model', BalancedRandomForestClassifier(random_state=SEED, n_jobs=-1))\n])\n\nparam_dist = {\n    'model__n_estimators': [200, 300, 400, 500, 700],\n    'model__max_depth': [None, 4, 6, 8, 12, 16],\n    'model__min_samples_leaf': [1, 2, 4, 8],\n    'model__max_features': ['sqrt', 'log2', 0.5, 0.8],\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nbrf_search = RandomizedSearchCV(\n    brf_pipe,\n    param_distributions=param_dist,\n    n_iter=30,\n    scoring='average_precision',\n    cv=cv,\n    random_state=SEED,\n    n_jobs=-1,\n    verbose=1,\n)\nbrf_search.fit(X_train, y_train)\n\nbest_brf = brf_search.best_estimator_\nbrf_valid_proba = best_brf.predict_proba(X_valid)[:,1]\nprint('Best BRF params:', brf_search.best_params_)\nprint('Best BRF CV AP:', brf_search.best_score_)\nprint('BRF validation AP:', average_precision_score(y_valid, brf_valid_proba))\nprint('BRF validation ROC-AUC:', roc_auc_score(y_valid, brf_valid_proba))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5B) LightGBM tuning with Optuna integration (LightGBMTunerCV)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X_train_lgb = preprocessor_tree.fit_transform(X_train)\nX_valid_lgb = preprocessor_tree.transform(X_valid)\nX_test_lgb = preprocessor_tree.transform(X_test)\n\nif sparse.issparse(X_train_lgb):\n    X_train_lgb = X_train_lgb.tocsr()\n    X_valid_lgb = X_valid_lgb.tocsr()\n    X_test_lgb = X_test_lgb.tocsr()\n\nlgb_train = lgb.Dataset(X_train_lgb, label=y_train)\n\ndef lgb_avg_precision(preds, data):\n    y_true = data.get_label()\n    return 'avg_precision', average_precision_score(y_true, preds), True\n\nparams = {\n    'objective': 'binary',\n    'metric': 'None',\n    'verbosity': -1,\n    'seed': SEED,\n    'feature_pre_filter': False,\n}\n\ntuner = optuna_lgb.LightGBMTunerCV(\n    params=params,\n    train_set=lgb_train,\n    folds=StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED),\n    feval=lgb_avg_precision,\n    num_boost_round=2000,\n    early_stopping_rounds=100,\n    optuna_seed=SEED,\n    time_budget=900,\n    return_cvbooster=True,\n    verbose_eval=False,\n)\ntuner.run()\n\nbest_params = tuner.best_params\nbest_iter = tuner.best_iteration\nprint('Best LGBM params:', best_params)\nprint('Best iteration:', best_iter)\n\nfinal_lgb = lgb.train(best_params, lgb_train, num_boost_round=best_iter)\nlgb_valid_proba = final_lgb.predict(X_valid_lgb)\nprint('LGBM validation AP:', average_precision_score(y_valid, lgb_valid_proba))\nprint('LGBM validation ROC-AUC:', roc_auc_score(y_valid, lgb_valid_proba))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Note:** some LightGBM integration flows emphasize built-in metrics. Here we pass custom AP `feval` and still use external holdout AP for final selection."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6 — Calibration + thresholding (no revenue/profit estimation)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Model selection uses VALIDATION AP only (no test leakage)\nvalid_candidates = {\n    'BRF_tuned': brf_valid_proba,\n    'LGBM_tuned': lgb_valid_proba,\n}\nbest_model_name = max(valid_candidates, key=lambda n: average_precision_score(y_valid, valid_candidates[n]))\nprint('Selected by validation AP:', best_model_name)\n\n# Refit selected model on train+valid, then evaluate ONCE on untouched test\nX_train_full = pd.concat([X_train, X_valid], axis=0)\ny_train_full = pd.concat([y_train, y_valid], axis=0)\n\nif best_model_name == 'BRF_tuned':\n    final_model = clone(best_brf)\n    final_model.fit(X_train_full, y_train_full)\n    test_scores = final_model.predict_proba(X_test)[:,1]\nelse:\n    X_train_full_lgb = preprocessor_tree.fit_transform(X_train_full)\n    X_test_full_lgb = preprocessor_tree.transform(X_test)\n    if sparse.issparse(X_train_full_lgb):\n        X_train_full_lgb = X_train_full_lgb.tocsr()\n        X_test_full_lgb = X_test_full_lgb.tocsr()\n    final_model = lgb.train(best_params, lgb.Dataset(X_train_full_lgb, label=y_train_full), num_boost_round=best_iter)\n    test_scores = final_model.predict(X_test_full_lgb)\n\nprint('FINAL TEST AP:', average_precision_score(y_test, test_scores))\nprint('FINAL TEST ROC-AUC:', roc_auc_score(y_test, test_scores))\n\nfor k in [0.01, 0.05, 0.10]:\n    print(precision_recall_at_k(y_test.values, test_scores, k))\n\ndef threshold_for_targeting_rate(scores, targeting_rate):\n    k = max(1, int(np.floor(len(scores)*targeting_rate)))\n    return float(np.sort(scores)[-k])\n\nrate = 0.05\nthr = threshold_for_targeting_rate(test_scores, rate)\nprint(f'Top {rate:.0%} targeting threshold:', thr)\nprint('At this targeting rate:', precision_recall_at_k(y_test.values, test_scores, rate))"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Optional calibration check\nprob_true, prob_pred = calibration_curve(y_test, test_scores, n_bins=10, strategy='quantile')\nbrier = brier_score_loss(y_test, test_scores)\n\nplt.figure(figsize=(6,5))\nplt.plot(prob_pred, prob_true, marker='o', label='Model')\nplt.plot([0,1],[0,1],'k--', label='Perfect calibration')\nplt.title(f'Reliability Curve (Brier={brier:.4f})')\nplt.xlabel('Predicted probability')\nplt.ylabel('Observed frequency')\nplt.legend(); plt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7 — Interpretability"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Logistic coefficients (fit on train+valid for interpretability)\nlogit_pipe = Pipeline([\n    ('prep', preprocessor_scaled),\n    ('model', LogisticRegression(max_iter=3000, class_weight='balanced', random_state=SEED))\n])\nlogit_pipe.fit(X_train_full, y_train_full)\nfeat_names = logit_pipe.named_steps['prep'].get_feature_names_out()\ncoefs = logit_pipe.named_steps['model'].coef_.ravel()\ncoef_df = pd.DataFrame({'feature': feat_names, 'coef': coefs})\nplot_df = pd.concat([coef_df.nsmallest(10, 'coef'), coef_df.nlargest(10, 'coef')])\n\nplt.figure(figsize=(10,7))\ncolors = ['#C44E52' if x < 0 else '#55A868' for x in plot_df['coef']]\nplt.barh(plot_df['feature'], plot_df['coef'], color=colors)\nplt.title('Logistic Regression Coefficients (Top +/-)')\nplt.xlabel('Coefficient')\nplt.tight_layout(); plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# SHAP summary for final LightGBM (when selected), otherwise for a reference LGBM fit\nif best_model_name != 'LGBM_tuned':\n    X_train_full_lgb = preprocessor_tree.fit_transform(X_train_full)\n    if sparse.issparse(X_train_full_lgb):\n        X_train_full_lgb = X_train_full_lgb.tocsr()\n    ref_lgb = lgb.train({'objective':'binary','verbosity':-1,'seed':SEED}, lgb.Dataset(X_train_full_lgb, label=y_train_full), num_boost_round=300)\n    shap_model = ref_lgb\n    X_shap_pool = X_train_full_lgb\nelse:\n    shap_model = final_model\n    X_shap_pool = preprocessor_tree.transform(X_test)\n    if sparse.issparse(X_shap_pool):\n        X_shap_pool = X_shap_pool.tocsr()\n\nsample_n = min(2000, X_shap_pool.shape[0])\nidx = np.random.choice(X_shap_pool.shape[0], size=sample_n, replace=False)\nX_shap = X_shap_pool[idx]\n\nexplainer = shap.TreeExplainer(shap_model)\nshap_values = explainer.shap_values(X_shap)\nif isinstance(shap_values, list):\n    shap_values = shap_values[1]\n\nshap.summary_plot(shap_values, X_shap, feature_names=preprocessor_tree.get_feature_names_out(), show=True)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Business interpretation:** use the top model’s SHAP ranking and logistic signs to identify which user patterns increase conversion propensity. Execute campaigns with top-K thresholds (e.g., 5%) for controlled outreach volume."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8 — Save artifacts + inference example"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Persist artifacts\njoblib.dump(preprocessor_scaled, 'preprocessor_scaled.joblib')\njoblib.dump(preprocessor_tree, 'preprocessor_tree.joblib')\njoblib.dump(best_brf, 'balanced_random_forest_tuned.joblib')\n\nif best_model_name == 'LGBM_tuned':\n    final_model.save_model('lightgbm_tuned.txt')\nelse:\n    # Save a reference lgbm too for reproducibility if BRF selected\n    X_train_full_lgb = preprocessor_tree.fit_transform(X_train_full)\n    if sparse.issparse(X_train_full_lgb):\n        X_train_full_lgb = X_train_full_lgb.tocsr()\n    ref_lgb = lgb.train({'objective':'binary','verbosity':-1,'seed':SEED}, lgb.Dataset(X_train_full_lgb, label=y_train_full), num_boost_round=300)\n    ref_lgb.save_model('lightgbm_reference.txt')\n\nprint('Artifacts saved.')"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Inference output: top-k propensity users in TEST (id + probability)\nid_test = work_df.loc[idx_test, [ID_COL]].reset_index(drop=True)\nout = id_test.copy()\nout['propensity_score'] = test_scores\nout = out.sort_values('propensity_score', ascending=False).reset_index(drop=True)\n\nk_show = 20\ndisplay(out.head(k_show))"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}