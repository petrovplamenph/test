{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Insurance Conversion Propensity Modeling (Colab)\n\nThis notebook is designed to run end-to-end in Google Colab for ranking users by conversion propensity (`has_sale`) with **Average Precision (PR-AUC)** as the primary model selection metric."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0 — Setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Colab setup (safe to run in local too)\n!pip -q install lightgbm imbalanced-learn optuna optuna-integration shap joblib\n\nimport os\nimport random\nimport warnings\nfrom dataclasses import dataclass\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport shap\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom optuna.integration import lightgbm as optuna_lgb\nfrom scipy import sparse\nfrom sklearn.base import clone\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    average_precision_score,\n    brier_score_loss,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nwarnings.filterwarnings('ignore')\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n\nplt.style.use('seaborn-v0_8-whitegrid')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1 — Load + sanity checks"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "DATA_PATH = 'data_(2).csv'\ndf = pd.read_csv(DATA_PATH)\ndf['dt'] = pd.to_datetime(df['dt'])\n\nprint('Shape:', df.shape)\nprint('\nDtypes:\n', df.dtypes)\nmissing_tbl = df.isna().mean().sort_values(ascending=False).rename('missing_rate')\nprint('\nMissingness:\n', (missing_tbl * 100).round(2).astype(str) + '%')\n\nbefore_n = len(df)\nbefore_pos = df['has_sale'].sum()\n\ndup_mask = df.duplicated(keep='first')\nnum_dups = int(dup_mask.sum())\ndf = df.loc[~dup_mask].copy()\n\nafter_n = len(df)\nafter_pos = df['has_sale'].sum()\n\nprint(f\"\\nRemoved duplicates: {num_dups}\")\nprint(f\"Rows: {before_n} -> {after_n}\")\nprint(f\"Positive count: {before_pos} -> {after_pos}\")\nprint(f\"Positive rate: {before_pos / before_n:.4f} -> {after_pos / after_n:.4f}\")\n\nassert num_dups == 164, f'Expected 164 duplicates from data facts, got {num_dups}'\n\nuniq_comm = df['commission_rate'].nunique(dropna=False)\nprint('commission_rate unique values:', uniq_comm)\nprint('commission_rate values:', df['commission_rate'].dropna().unique()[:5])\nassert uniq_comm == 1, 'commission_rate should be constant per provided data facts.'\n\ndf = df.drop(columns=['commission_rate'])\nprint('Dropped commission_rate. New shape:', df.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2 — EDA with figures"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Target count and rate\ncounts = df['has_sale'].value_counts().sort_index()\naxes[0].bar(['No Sale (0)', 'Sale (1)'], counts.values, color=['#4C72B0', '#55A868'])\naxes[0].set_title('Target Counts')\naxes[0].set_ylabel('Count')\n\nrate = df['has_sale'].mean()\naxes[1].bar(['Conversion Rate'], [rate], color='#55A868')\naxes[1].set_ylim(0, 1)\naxes[1].set_title(f'Conversion Rate: {rate:.2%}')\naxes[1].set_ylabel('Rate')\n\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Insight:** Conversion is imbalanced (~22% positive), so model comparison should prioritize ranking quality metrics like PR-AUC rather than accuracy."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "missing = df.isna().mean().sort_values(ascending=False)\nmissing = missing[missing > 0]\n\nplt.figure(figsize=(8, 4))\nplt.bar(missing.index, missing.values * 100, color='#C44E52')\nplt.title('Missingness by Column (%)')\nplt.ylabel('Missing %')\nplt.xticks(rotation=30, ha='right')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "platform_plot = df.copy()\nplatform_plot['platform'] = platform_plot['platform'].fillna('Missing')\nplatform_rate = platform_plot.groupby('platform')['has_sale'].mean().sort_values(ascending=False)\n\nplt.figure(figsize=(6, 4))\nplt.bar(platform_rate.index, platform_rate.values, color='#8172B2')\nplt.title('Conversion Rate by Platform')\nplt.ylabel('Conversion Rate')\nplt.ylim(0, max(platform_rate.values)*1.2)\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "member_plot = df.copy()\nmember_plot['membership_level'] = member_plot['membership_level'].fillna('Missing')\nmember_rate = member_plot.groupby('membership_level')['has_sale'].mean().sort_values(ascending=False)\n\nplt.figure(figsize=(7, 4))\nplt.bar(member_rate.index, member_rate.values, color='#64B5CD')\nplt.title('Conversion Rate by Membership Level (with Missing)')\nplt.ylabel('Conversion Rate')\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "num_cols = ['age', 'monthly_cost', 'session_time', 'household_income']\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, col in zip(axes, num_cols):\n    ax.hist(df.loc[df['has_sale'] == 0, col], bins=30, alpha=0.55, label='No Sale', density=True)\n    ax.hist(df.loc[df['has_sale'] == 1, col], bins=30, alpha=0.55, label='Sale', density=True)\n    ax.set_title(f'{col} distribution by target')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Quantile-binned conversion rate vs numeric features\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, col in zip(axes, num_cols):\n    tmp = df[[col, 'has_sale']].copy()\n    tmp['bin'] = pd.qcut(tmp[col], q=10, duplicates='drop')\n    agg = tmp.groupby('bin')['has_sale'].mean().reset_index()\n    ax.plot(range(len(agg)), agg['has_sale'], marker='o')\n    ax.set_title(f'Binned conversion rate vs {col}')\n    ax.set_xlabel('Quantile bin index')\n    ax.set_ylabel('Conversion rate')\n\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Insights:** `platform` is a strong separator; mid-range age appears to convert more than extremes; lower `monthly_cost` segments appear to convert better. These are ranking signals to capture in models."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3 — Feature engineering + preprocessing (sklearn Pipeline)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "EPS = 1e-6\n\ndef add_features(frame: pd.DataFrame) -> pd.DataFrame:\n    out = frame.copy()\n    out['day_of_week'] = out['dt'].dt.dayofweek\n    out['is_weekend'] = (out['day_of_week'] >= 5).astype(int)\n    out['day_of_month'] = out['dt'].dt.day\n    out['month'] = out['dt'].dt.month\n    out['cost_income_ratio'] = out['monthly_cost'] / np.maximum(out['household_income'].values, EPS)\n    return out\n\nwork_df = add_features(df)\n\nTARGET = 'has_sale'\nID_COL = 'id'\nDROP_COLS = [TARGET, ID_COL, 'dt']\n\nX = work_df.drop(columns=DROP_COLS)\ny = work_df[TARGET].astype(int)\n\nnum_features = X.select_dtypes(include=['number']).columns.tolist()\ncat_features = X.select_dtypes(exclude=['number']).columns.tolist()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler()),\n        ]), num_features),\n        ('cat', Pipeline([\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('ohe', OneHotEncoder(handle_unknown='ignore')),\n        ]), cat_features),\n    ]\n)\n\nprint('Numeric features:', num_features)\nprint('Categorical features:', cat_features)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4 — Modeling & evaluation (ranking-focused)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@dataclass\nclass EvalResult:\n    model_name: str\n    pr_auc: float\n    roc_auc: float\n\n\ndef precision_recall_at_k(y_true, y_score, k_frac):\n    n = len(y_true)\n    k = max(1, int(np.floor(n * k_frac)))\n    order = np.argsort(-y_score)\n    top_idx = order[:k]\n    y_top = np.asarray(y_true)[top_idx]\n    precision_k = y_top.mean()\n    recall_k = y_top.sum() / np.asarray(y_true).sum()\n    threshold = np.sort(y_score)[-k]\n    return {'k_frac': k_frac, 'k': k, 'threshold': float(threshold), 'precision@k': float(precision_k), 'recall@k': float(recall_k)}\n\n\ndef evaluate_scores(y_true, y_score):\n    return average_precision_score(y_true, y_score), roc_auc_score(y_true, y_score)\n\n# Stratified split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=SEED\n)\n\nmodels = {\n    'LogisticRegression': Pipeline([\n        ('prep', preprocessor),\n        ('model', LogisticRegression(max_iter=3000, class_weight='balanced', random_state=SEED))\n    ]),\n    'BalancedRandomForest': Pipeline([\n        ('prep', preprocessor),\n        ('model', BalancedRandomForestClassifier(\n            n_estimators=400, min_samples_leaf=2, random_state=SEED, n_jobs=-1\n        ))\n    ]),\n    'LGBM_baseline': Pipeline([\n        ('prep', preprocessor),\n        ('model', LGBMClassifier(\n            n_estimators=400, learning_rate=0.05, num_leaves=31,\n            class_weight='balanced', random_state=SEED, n_jobs=-1, verbosity=-1\n        ))\n    ]),\n}\n\nresults = []\ntest_scores = {}\nfor name, pipe in models.items():\n    pipe.fit(X_train, y_train)\n    proba = pipe.predict_proba(X_test)[:, 1]\n    pr, roc = evaluate_scores(y_test, proba)\n    results.append(EvalResult(name, pr, roc))\n    test_scores[name] = proba\n\nres_df = pd.DataFrame([r.__dict__ for r in results]).sort_values('pr_auc', ascending=False)\nprint(res_df)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Alternative time-based split (earlier -> later)\ntime_sorted = work_df.sort_values('dt').reset_index(drop=True)\ncut = int(len(time_sorted) * 0.8)\ntrain_time = time_sorted.iloc[:cut]\ntest_time = time_sorted.iloc[cut:]\n\nX_train_t = train_time.drop(columns=DROP_COLS)\ny_train_t = train_time[TARGET].astype(int)\nX_test_t = test_time.drop(columns=DROP_COLS)\ny_test_t = test_time[TARGET].astype(int)\n\ntime_compare = []\nfor name, pipe in models.items():\n    pipe_t = clone(pipe)\n    pipe_t.fit(X_train_t, y_train_t)\n    p = pipe_t.predict_proba(X_test_t)[:, 1]\n    pr, roc = evaluate_scores(y_test_t, p)\n    time_compare.append({'model': name, 'time_split_pr_auc': pr, 'time_split_roc_auc': roc})\n\nprint(pd.DataFrame(time_compare).sort_values('time_split_pr_auc', ascending=False))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Curves on stratified test split for baseline models\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nfor name, score in test_scores.items():\n    precision, recall, _ = precision_recall_curve(y_test, score)\n    plt.plot(recall, precision, label=f\"{name} (AP={average_precision_score(y_test, score):.3f})\")\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curves')\nplt.legend()\n\nplt.subplot(1,2,2)\nfor name, score in test_scores.items():\n    fpr, tpr, _ = roc_curve(y_test, score)\n    plt.plot(fpr, tpr, label=f\"{name} (ROC-AUC={roc_auc_score(y_test, score):.3f})\")\nplt.plot([0,1],[0,1],'k--',alpha=0.5)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curves')\nplt.legend()\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5 — Hyperparameter tuning (optimize PR-AUC)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5A) Balanced Random Forest tuning (RandomizedSearchCV on Average Precision)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "brf_pipe = Pipeline([\n    ('prep', preprocessor),\n    ('model', BalancedRandomForestClassifier(random_state=SEED, n_jobs=-1))\n])\n\nparam_dist = {\n    'model__n_estimators': [200, 300, 400, 500, 700],\n    'model__max_depth': [None, 4, 6, 8, 12, 16],\n    'model__min_samples_leaf': [1, 2, 4, 8],\n    'model__max_features': ['sqrt', 'log2', 0.5, 0.8],\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\nbrf_search = RandomizedSearchCV(\n    brf_pipe,\n    param_distributions=param_dist,\n    n_iter=30,\n    scoring='average_precision',\n    cv=cv,\n    random_state=SEED,\n    n_jobs=-1,\n    verbose=1,\n)\nbrf_search.fit(X_train, y_train)\n\nbest_brf = brf_search.best_estimator_\nbrf_test_proba = best_brf.predict_proba(X_test)[:, 1]\nprint('Best BRF params:', brf_search.best_params_)\nprint('Best BRF CV AP:', brf_search.best_score_)\nprint('Best BRF test AP:', average_precision_score(y_test, brf_test_proba))\nprint('Best BRF test ROC-AUC:', roc_auc_score(y_test, brf_test_proba))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5B) LightGBM tuning with Optuna integration (LightGBMTunerCV)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Prepare transformed matrices for LightGBM tuner\nprep_no_scale = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([\n            ('imputer', SimpleImputer(strategy='median')),\n        ]), num_features),\n        ('cat', Pipeline([\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('ohe', OneHotEncoder(handle_unknown='ignore')),\n        ]), cat_features),\n    ]\n)\n\nX_train_lgb = prep_no_scale.fit_transform(X_train)\nX_test_lgb = prep_no_scale.transform(X_test)\n\nif sparse.issparse(X_train_lgb):\n    X_train_lgb = X_train_lgb.tocsr()\n    X_test_lgb = X_test_lgb.tocsr()\n\nlgb_train = lgb.Dataset(X_train_lgb, label=y_train)\n\n# custom AP evaluator for CV\n\ndef lgb_avg_precision(preds, data):\n    y_true = data.get_label()\n    return 'avg_precision', average_precision_score(y_true, preds), True\n\nparams = {\n    'objective': 'binary',\n    'metric': 'None',\n    'verbosity': -1,\n    'seed': SEED,\n    'feature_pre_filter': False,\n}\n\ntuner = optuna_lgb.LightGBMTunerCV(\n    params=params,\n    train_set=lgb_train,\n    num_boost_round=2000,\n    folds=StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED),\n    feval=lgb_avg_precision,\n    early_stopping_rounds=100,\n    optuna_seed=SEED,\n    time_budget=900,  # 15 min budget; adjust in Colab if needed\n    return_cvbooster=True,\n    verbose_eval=False,\n)\n\ntuner.run()\nbest_params = tuner.best_params\nprint('Best params from LightGBMTunerCV:', best_params)\n\n# Train final model using tuned params\nbest_num_boost_round = tuner.best_iteration\nfinal_lgb = lgb.train(\n    params=best_params,\n    train_set=lgb_train,\n    num_boost_round=best_num_boost_round,\n)\n\nlgb_test_proba = final_lgb.predict(X_test_lgb)\nprint('Tuned LGBM test AP:', average_precision_score(y_test, lgb_test_proba))\nprint('Tuned LGBM test ROC-AUC:', roc_auc_score(y_test, lgb_test_proba))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Note:** Some LightGBM/Optuna integration versions prioritize built-in metrics; here we pass a custom `feval` for AP and still validate AP externally on the holdout set for final model selection."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6 — Calibration + thresholding (operational)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Select final model by highest test AP among tuned BRF and tuned LGBM\ncandidate_scores = {\n    'BRF_tuned': brf_test_proba,\n    'LGBM_tuned': lgb_test_proba,\n}\nfinal_name = max(candidate_scores, key=lambda n: average_precision_score(y_test, candidate_scores[n]))\nfinal_scores = candidate_scores[final_name]\n\nprint('Selected final model by AP:', final_name)\nprint('Final AP:', average_precision_score(y_test, final_scores))\nprint('Final ROC-AUC:', roc_auc_score(y_test, final_scores))\n\nfor k in [0.01, 0.05, 0.10]:\n    out = precision_recall_at_k(y_test.values, final_scores, k)\n    print(out)\n\n\ndef threshold_for_targeting_rate(scores, targeting_rate):\n    k = max(1, int(np.floor(len(scores) * targeting_rate)))\n    return float(np.sort(scores)[-k])\n\nexample_rate = 0.05\nthr = threshold_for_targeting_rate(final_scores, example_rate)\nmetrics_at_5 = precision_recall_at_k(y_test.values, final_scores, example_rate)\nprint(f\"\\nAt top {example_rate:.0%} targeting, threshold={thr:.5f}\")\nprint(metrics_at_5)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Optional calibration check\nprob_true, prob_pred = calibration_curve(y_test, final_scores, n_bins=10, strategy='quantile')\nbrier = brier_score_loss(y_test, final_scores)\n\nplt.figure(figsize=(6,5))\nplt.plot(prob_pred, prob_true, marker='o', label='Model')\nplt.plot([0,1],[0,1],'k--', label='Perfectly calibrated')\nplt.xlabel('Predicted probability')\nplt.ylabel('Observed frequency')\nplt.title(f'Reliability Curve (Brier={brier:.4f})')\nplt.legend()\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7 — Interpretability"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Logistic regression coefficient plot (top +/-)\nlogit = models['LogisticRegression']\nlogit.fit(X_train, y_train)\nfeature_names = logit.named_steps['prep'].get_feature_names_out()\ncoefs = logit.named_steps['model'].coef_.ravel()\ncoef_df = pd.DataFrame({'feature': feature_names, 'coef': coefs})\n\ntop_pos = coef_df.nlargest(10, 'coef')\ntop_neg = coef_df.nsmallest(10, 'coef')\nplot_df = pd.concat([top_neg, top_pos], axis=0)\n\nplt.figure(figsize=(10, 7))\ncolors = ['#C44E52' if c < 0 else '#55A868' for c in plot_df['coef']]\nplt.barh(plot_df['feature'], plot_df['coef'], color=colors)\nplt.title('Logistic Regression: Top Positive/Negative Coefficients')\nplt.xlabel('Coefficient value')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# SHAP for tuned LightGBM model\n# For speed/readability, use a sample\nsample_n = min(2000, X_test_lgb.shape[0])\nidx = np.random.choice(X_test_lgb.shape[0], size=sample_n, replace=False)\nX_shap = X_test_lgb[idx]\n\nexplainer = shap.TreeExplainer(final_lgb)\nshap_values = explainer.shap_values(X_shap)\n\n# binary classifiers may return list [class0, class1]\nif isinstance(shap_values, list):\n    shap_values_to_plot = shap_values[1]\nelse:\n    shap_values_to_plot = shap_values\n\nfeature_names_lgb = prep_no_scale.get_feature_names_out()\nshap.summary_plot(shap_values_to_plot, X_shap, feature_names=feature_names_lgb, show=True)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Business interpretation (example):** prioritize users with feature patterns associated with higher model scores (from SHAP/coefs). Use top-K targeting (e.g., top 5%) to control campaign volume while maximizing conversion lift. This notebook does **not** estimate revenue/profit."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8 — Save artifacts + inference example"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Save artifacts\njoblib.dump(preprocessor, 'preprocessor_scaled.joblib')\njoblib.dump(prep_no_scale, 'preprocessor_lgb.joblib')\njoblib.dump(best_brf, 'balanced_random_forest_tuned.joblib')\nfinal_lgb.save_model('lightgbm_tuned.txt')\n\nprint('Artifacts saved.')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Inference example: top-k highest propensity users on the holdout test set\n# Build an ID-aligned test frame\n_, test_idx = train_test_split(\n    work_df.index,\n    test_size=0.2,\n    stratify=work_df[TARGET],\n    random_state=SEED,\n)\n\nid_test = work_df.loc[test_idx, [ID_COL]].copy().reset_index(drop=True)\nscore_df = id_test.copy()\nscore_df['propensity_score'] = final_scores\nscore_df = score_df.sort_values('propensity_score', ascending=False).reset_index(drop=True)\n\nk_show = 20\ndisplay(score_df.head(k_show))"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}